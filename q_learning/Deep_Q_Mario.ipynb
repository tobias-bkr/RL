{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spexx/Desktop/ssd/Code/ArtI/RL/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against ABI version 0x1000009 but this version of numpy is 0x2000000",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[31mRuntimeError\u001b[39m: module compiled against ABI version 0x1000009 but this version of numpy is 0x2000000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against ABI version 0x1000009 but this version of numpy is 0x2000000",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[31mRuntimeError\u001b[39m: module compiled against ABI version 0x1000009 but this version of numpy is 0x2000000"
     ]
    }
   ],
   "source": [
    "## IMPORTANT: This notebook does not work anymore since the super-mario-bros package is unmaintained\n",
    "\n",
    "import random\n",
    "import json\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "\n",
    "torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, maxLen):\n",
    "        self.Buffer = deque([], maxlen=maxLen)\n",
    "\n",
    "    def add(self,data):\n",
    "        self.Buffer.append(data)\n",
    "        return\n",
    "    \n",
    "    def sample(self, batchSize):\n",
    "        # returns a list with the batches as a list at each index\n",
    "        x = random.sample(self.Buffer, batchSize)\n",
    "        return list(zip(*x))\n",
    "    \n",
    "    def altSample(self, batchSize):\n",
    "        # might work, might not\n",
    "        indices = random.sample(range(len(self.Buffer)), batchSize)\n",
    "        batch = [[self.Buffer[x][y] for x in indices] for y in range(len(self.Buffer[0]))]\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, inFrames, outputDimension):\n",
    "        super().__init__()\n",
    "        # initializes layers with kaiming uniform\n",
    "        self.Conv2d_1 = torch.nn.Conv2d(in_channels=inFrames, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.Conv2d_2 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.Conv2d_3 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.Linear_1 = torch.nn.Linear(3136, 512) \n",
    "        self.Linear_2 = torch.nn.Linear(512, outputDimension)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input (1,84,84)\n",
    "        x = self.Conv2d_1(input)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.Conv2d_2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.Conv2d_3(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = x.flatten()\n",
    "        x = self.Linear_1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.Linear_2(x)\n",
    "        # output (outputDimension)\n",
    "        return x\n",
    "    \n",
    "    def forwardBatch(self, input):\n",
    "        # input (Batch,1,84,84)\n",
    "        x = self.Conv2d_1(input)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.Conv2d_2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.Conv2d_3(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        x = self.Linear_1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.Linear_2(x)\n",
    "        # output (Batch, outputDimension)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickAction(state):\n",
    "    global Epsilon\n",
    "\n",
    "    if(Epsilon > EpsilonEnd):\n",
    "        Epsilon = Epsilon - ((EpsilonStart-EpsilonEnd)/EpsilonStepsbetweenStartandEnd)\n",
    "    if(random.random()<Epsilon):\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        prediction = MarioPlayer(state)\n",
    "        average_q.append(torch.max(prediction).item())\n",
    "        return torch.argmax(prediction).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FrameskipStep(env, action, SkipFrames):\n",
    "    fullReward = 0\n",
    "    for _ in range(SkipFrames):\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        fullReward += reward\n",
    "        if(terminated or truncated):\n",
    "            return observation, fullReward, terminated, truncated, info\n",
    "    return observation, fullReward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Visualization(StateBatch, actionBatch, rewardBatch, NextStateBatch, predictions, nextStateprediction, target):\n",
    "    # input / output vizualization\n",
    "    # state\n",
    "    fig, ax = plt.subplots(1, inFrames, figsize=(13, 10))\n",
    "    for i in range(inFrames):\n",
    "        ax[i].imshow(StateBatch[0][i].cpu(), cmap='gray', vmin=0, vmax=255)\n",
    "        plt.tight_layout()\n",
    "    plt.show()\n",
    "    # predictions for the moves\n",
    "    print(\"Predictions for the moves:\\nNOOP, RIGHT, RIGHT + JUMP, RIGHT + RUN, RIGHT + RUN + JUMP, JUMP, LEFT\")\n",
    "    print(predictions[0].cpu())\n",
    "    # actual move\n",
    "    print(\"Actual move (might be different from highest predicted value because decision was made by old model or randomly)\")\n",
    "    print(actionBatch[0].cpu())\n",
    "    # nextState\n",
    "    fig, ax = plt.subplots(1, inFrames, figsize=(13, 10))\n",
    "    for i in range(inFrames):\n",
    "        plt.imshow(NextStateBatch[0][i].cpu(), cmap='gray', vmin=0, vmax=255)\n",
    "        ax[i].imshow(NextStateBatch[0][i].cpu(), cmap='gray', vmin=0, vmax=255)\n",
    "        plt.tight_layout()\n",
    "    plt.show()\n",
    "    # nextStatePredictions\n",
    "    print(\"Prediction for the moves in the next State:\\nNOOP, RIGHT, RIGHT + JUMP, RIGHT + RUN, RIGHT + RUN + JUMP, JUMP, LEFT\")\n",
    "    print(nextStateprediction[0].cpu())\n",
    "    # actual reward\n",
    "    print(\"Actual reward\")\n",
    "    print(rewardBatch[0])\n",
    "    # target\n",
    "    print(\"Target (reward + gamma*Highest predicted action value in NextState)\")\n",
    "    print(target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Optimizer(MarioPlayer, MarioBuffer, BatchSize, optimizer, lossFunction):\n",
    "    # TD Learning\n",
    "    if (len(MarioBuffer.Buffer)<BatchSize):\n",
    "        return\n",
    "    Batches = MarioBuffer.sample(BatchSize)\n",
    "    StateBatch = torch.cat(Batches[0])\n",
    "    actionBatch = torch.cat(Batches[1])\n",
    "    rewardBatch = torch.cat(Batches[2])\n",
    "    NextStateBatch = torch.cat(Batches[3])\n",
    "    terminated_batch = torch.cat(Batches[4])\n",
    "\n",
    "    # zeroes the gradients because default behaviour in PT is to accumulate them\n",
    "    for param in MarioPlayer.parameters():\n",
    "        param.grad = None\n",
    "\n",
    "    predictions = MarioPlayer.forwardBatch(StateBatch)\n",
    "    prediction = predictions.gather(1, actionBatch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        nextStateprediction = MarioPredictor.forwardBatch(NextStateBatch)\n",
    "        nextStatepredictionMax = nextStateprediction.max(1).values.unsqueeze(1)\n",
    "        target = rewardBatch + gamma*(nextStatepredictionMax*terminated_batch)\n",
    "\n",
    "    if(i%10000 == 0):\n",
    "        print(f\"{i}\\n\")\n",
    "        Visualization(StateBatch, actionBatch, rewardBatch, NextStateBatch, predictions, nextStateprediction, target)\n",
    "\n",
    "    loss = lossFunction(prediction,target)\n",
    "    if(i%100 == 0):\n",
    "        writer.add_scalar(\"Loss/Batch\", loss.item(), global_step=i)\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(MarioPlayer.parameters(), 10)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "EpsilonStart = 0.1\n",
    "Epsilon = EpsilonStart\n",
    "EpsilonStepsbetweenStartandEnd = 1_000_000\n",
    "EpsilonEnd = 0.1\n",
    "\n",
    "gamma = 0.99\n",
    "BatchSize = 32\n",
    "\n",
    "inFrames = 4\n",
    "possibleActions = 7\n",
    "# the agent only sees every nth frame (1, 1+n, 1+2n, etc.) so the agent doesnt see the n-1 frames in between\n",
    "SkipFrames = 6\n",
    "# 1 means optimization every step, 2 every 2nd step etc.\n",
    "steps_per_optimization = 1\n",
    "steps_per_target_net_update = 10_000\n",
    "\n",
    "\n",
    "BufferLength = 100_000\n",
    "\n",
    "agent = \"Mario_DQN_3\"\n",
    "\n",
    "logdir = f\"./logs/{agent}\"\n",
    "\n",
    "MarioPlayer = Network(inFrames,possibleActions)\n",
    "# MarioPlayer.load_state_dict(torch.load(f\"./models/{agent}\", weights_only=True))\n",
    "\n",
    "MarioPredictor = Network(inFrames,possibleActions)\n",
    "MarioPredictor.load_state_dict(MarioPlayer.state_dict())\n",
    "\n",
    "MarioBuffer = ReplayBuffer(BufferLength)\n",
    "\n",
    "optimizer = torch.optim.Adam(MarioPlayer.parameters(), lr=1e-4)\n",
    "\n",
    "# what reduction to use? if you sum the gradients are bigger (also depend on batchSize then)\n",
    "# just use mean for now\n",
    "lossFunction = torch.nn.SmoothL1Loss()\n",
    "\n",
    "trainingSteps = 10_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spexx/Desktop/ssd/Code/ArtI/RL/.venv/lib/python3.11/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3` with the environment ID `SuperMarioBros-v3`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "Python integer 1024 out of bounds for uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOverflowError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m writer = SummaryWriter(log_dir=logdir)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# sampling trajectories loop\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m env = \u001b[43mgym_super_mario_bros\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSuperMarioBros-v0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m env = JoypadSpace(env, SIMPLE_MOVEMENT)\n\u001b[32m      9\u001b[39m env = gym.wrappers.ResizeObservation(env, (\u001b[32m84\u001b[39m,\u001b[32m84\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ssd/Code/ArtI/RL/.venv/lib/python3.11/site-packages/gym/envs/registration.py:676\u001b[39m, in \u001b[36mmake\u001b[39m\u001b[34m(id, **kwargs)\u001b[39m\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmake\u001b[39m(\u001b[38;5;28mid\u001b[39m: \u001b[38;5;28mstr\u001b[39m, **kwargs) -> \u001b[33m\"\u001b[39m\u001b[33mEnv\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m676\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ssd/Code/ArtI/RL/.venv/lib/python3.11/site-packages/gym/envs/registration.py:520\u001b[39m, in \u001b[36mEnvRegistry.make\u001b[39m\u001b[34m(self, path, **kwargs)\u001b[39m\n\u001b[32m    518\u001b[39m spec = \u001b[38;5;28mself\u001b[39m.spec(path)\n\u001b[32m    519\u001b[39m \u001b[38;5;66;03m# Construct the environment\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ssd/Code/ArtI/RL/.venv/lib/python3.11/site-packages/gym/envs/registration.py:140\u001b[39m, in \u001b[36mEnvSpec.make\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28mcls\u001b[39m = load(\u001b[38;5;28mself\u001b[39m.entry_point)\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     env = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# Make the environment aware of which spec it came from.\u001b[39;00m\n\u001b[32m    143\u001b[39m spec = copy.deepcopy(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ssd/Code/ArtI/RL/.venv/lib/python3.11/site-packages/gym_super_mario_bros/smb_env.py:52\u001b[39m, in \u001b[36mSuperMarioBrosEnv.__init__\u001b[39m\u001b[34m(self, rom_mode, lost_levels, target)\u001b[39m\n\u001b[32m     50\u001b[39m rom = rom_path(lost_levels, rom_mode)\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# initialize the super object with the ROM path\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSuperMarioBrosEnv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrom\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# set the target world, stage, and area variables\u001b[39;00m\n\u001b[32m     54\u001b[39m target = decode_target(target, lost_levels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ssd/Code/ArtI/RL/.venv/lib/python3.11/site-packages/nes_py/nes_env.py:126\u001b[39m, in \u001b[36mNESEnv.__init__\u001b[39m\u001b[34m(self, rom_path)\u001b[39m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mROM has trainer. trainer is not supported.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# try to read the PRG ROM and raise a value error if it fails\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m _ = \u001b[43mrom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprg_rom\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# try to read the CHR ROM and raise a value error if it fails\u001b[39;00m\n\u001b[32m    128\u001b[39m _ = rom.chr_rom\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ssd/Code/ArtI/RL/.venv/lib/python3.11/site-packages/nes_py/_rom.py:204\u001b[39m, in \u001b[36mROM.prg_rom\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the PRG ROM of the ROM file.\"\"\"\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw_data[\u001b[38;5;28mself\u001b[39m.prg_rom_start:\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprg_rom_stop\u001b[49m]\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mfailed to read PRG-ROM on ROM.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ssd/Code/ArtI/RL/.venv/lib/python3.11/site-packages/nes_py/_rom.py:198\u001b[39m, in \u001b[36mROM.prg_rom_stop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprg_rom_stop\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    197\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"The exclusive stopping index of the PRG ROM.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prg_rom_start + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprg_rom_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m10\u001b[39;49m\n",
      "\u001b[31mOverflowError\u001b[39m: Python integer 1024 out of bounds for uint8"
     ]
    }
   ],
   "source": [
    "# logging setup\n",
    "\n",
    "writer = SummaryWriter(log_dir=logdir)\n",
    "\n",
    "# sampling trajectories loop\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "env = gym.wrappers.ResizeObservation(env, (84,84))\n",
    "env = gym.wrappers.GrayScaleObservation(env, keep_dim = True)\n",
    "env = gym.wrappers.FrameStack(env, 4)\n",
    "\n",
    "observation, info = env.reset()\n",
    "for _ in range(random.randint(0,30)):\n",
    "    observation, reward, terminated, truncated, info = env.step(0)    \n",
    "\n",
    "nextState = torch.tensor(numpy.array(observation), dtype=torch.float32)\n",
    "nextState= nextState.reshape((1,4,84,84))\n",
    "\n",
    "episodeReward = 0\n",
    "average_q = []\n",
    "\n",
    "for i in range(trainingSteps):\n",
    "    state = nextState\n",
    "    action = pickAction(state)\n",
    "    observation, reward, terminated, truncated, info = FrameskipStep(env, action, SkipFrames)\n",
    "    if(info['flag_get'] == True):\n",
    "        reward += 15\n",
    "    episodeReward += reward\n",
    "    nextState = torch.tensor(numpy.array(observation), dtype=torch.float32)\n",
    "    nextState = nextState.reshape((1,4,84,84))\n",
    "\n",
    "    MarioBuffer.add([state,torch.tensor([[action]]),torch.tensor([[reward]]),nextState, torch.tensor([[not (terminated or truncated)]])])\n",
    "\n",
    "    if(i % steps_per_optimization == 0):\n",
    "        Optimizer(MarioPlayer, MarioBuffer, BatchSize, optimizer, lossFunction)\n",
    "\n",
    "    if(i % steps_per_target_net_update == 0):\n",
    "        MarioPredictor.load_state_dict(MarioPlayer.state_dict())\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset() \n",
    "\n",
    "        for _ in range(random.randint(0,30)):\n",
    "            observation, reward, terminated, truncated, info = env.step(0)  \n",
    "\n",
    "        nextState = torch.tensor(numpy.array(observation), dtype=torch.float32)\n",
    "        nextState = nextState.reshape((1,4,84,84))\n",
    "\n",
    "        writer.add_scalar(\"Reward/Episode\", episodeReward, global_step=i)\n",
    "\n",
    "        episodeReward = 0\n",
    "        average_q_scalar = 0\n",
    "\n",
    "        if(len(average_q) > 0):\n",
    "            for j in average_q:\n",
    "                average_q_scalar += j\n",
    "            average_q_scalar /= len(average_q)\n",
    "        writer.add_scalar(\"Average_Q\", average_q_scalar, global_step=i)\n",
    "        average_q = []\n",
    "\n",
    "    if (i%100000 == 0):\n",
    "        torch.save(MarioPlayer.state_dict(), f\"./models/{agent}_{i}\")    \n",
    "\n",
    "env.close()\n",
    "\n",
    "# save model\n",
    "torch.save(MarioPlayer.state_dict(), f\"./models/{agent}\")\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epsilon = 0.05\n",
    "\n",
    "inFrames = 4\n",
    "possibleActions = 7\n",
    "# the agent only sees every nth frame (1, 1+n, 1+2n, etc.) so the agent doesnt see the n-1 frames in between\n",
    "SkipFrames = 6\n",
    "\n",
    "agent = \"Mario_DQN_2\"\n",
    "\n",
    "logdir = f\"logs/{agent}\"\n",
    "\n",
    "MarioPlayer = Network(inFrames,possibleActions)\n",
    "MarioPlayer.load_state_dict(torch.load(f\"./models/{agent}\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation enviroment\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-4-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "env = gym.wrappers.ResizeObservation(env, (84,84))\n",
    "env = gym.wrappers.GrayScaleObservation(env, keep_dim = True)\n",
    "env = gym.wrappers.FrameStack(env, 4)\n",
    "\n",
    "observation, info = env.reset()\n",
    "for _ in range(random.randint(0,30)):\n",
    "    observation, reward, terminated, truncated, info = env.step(0)  \n",
    "\n",
    "validationEpisodeReward = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    observation, info = env.reset()\n",
    "    nextState = torch.tensor(numpy.array(observation), dtype=torch.float32)\n",
    "    nextState= nextState.reshape((1,4,84,84))\n",
    "\n",
    "    terminated, truncated = False, False\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        state = nextState\n",
    "        action = pickAction(state)\n",
    "        observation, reward, terminated, truncated, info = FrameskipStep(env, action, SkipFrames)\n",
    "        env.render()\n",
    "        time.sleep(0.05)\n",
    "        validationEpisodeReward += reward\n",
    "        nextState = torch.tensor(numpy.array(observation), dtype=torch.float32)\n",
    "        nextState = nextState.reshape((1,4,84,84))\n",
    "\n",
    "print(validationEpisodeReward)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
